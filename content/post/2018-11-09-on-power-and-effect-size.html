---
title: On power and effect size
author: Darrin Speegle
date: '2018-11-09'
slug: on-power-and-effect-size
categories: []
tags:
  - power
  - effect size
  - simulation
---



<div id="introduction" class="section level3">
<h3>Introduction</h3>
<p>I have seen a lot of tweets on my feed about power and effect size. I wanted to think about those things carefully, so I did some reading and am writing down some thoughts. An interesting paper that I read on this is <a href="http://www.stat.columbia.edu/~gelman/research/unpublished/power.pdf">here</a> The takeaway is that studies that are underpowered, yet still obtain statistically significant results, will tend to overestimate the effect size.</p>
</div>
<div id="simulations" class="section level3">
<h3>Simulations</h3>
<p>Let’s imagine that we are doing a hypothesis test of <span class="math inline">\(\mu = 0\)</span> versus <span class="math inline">\(\mu \not = 0\)</span>, and our underlying population is normal with mean 1 and standard deviation 3. If we take a sample of size 30 and test at the <span class="math inline">\(\alpha = .05\)</span> level, then our power is about 42%. We note that the true (Cohen’s <span class="math inline">\(d\)</span>) effect size here is 1/3.</p>
<pre class="r"><code>power.t.test(n = 30, delta = 1, sd = 3, sig.level = .05, type = &quot;one&quot;)</code></pre>
<pre><code>## 
##      One-sample t test power calculation 
## 
##               n = 30
##           delta = 1
##              sd = 3
##       sig.level = 0.05
##           power = 0.4228091
##     alternative = two.sided</code></pre>
<p>OK, now let’s run some simulations. This takes a sample of size 30 from normal with mean 1 and sd 3, and does a t-test. If it is significant at the <span class="math inline">\(\alpha = .05\)</span> level, then the mean and sd are computed of the original data.</p>
<pre class="r"><code>dat &lt;- rnorm(30, 1, 3)
library(dplyr)
library(ggplot2)
sim_data &lt;- do.call(rbind.data.frame, lapply(1:10000, function(x) {
  dat &lt;- rnorm(30, 1, 3)
  if(t.test(dat)$p.value &lt; .05) { 
       data.frame(mean = mean(dat), sd = sd(dat))
  } else {
       data.frame(mean = NA, sd = NA)
  }
})) %&gt;% tidyr::drop_na()</code></pre>
<p>Now, let’s see what the Cohen’s <span class="math inline">\(d\)</span> values were, for the significant tests.</p>
<pre class="r"><code>sim_data &lt;- mutate(sim_data, cohensd = mean/sd)
hist(sim_data$cohensd)</code></pre>
<p><img src="/post/2018-11-09-on-power-and-effect-size_files/figure-html/unnamed-chunk-3-1.png" width="672" /> As we can see, the most likely outcome is just under 0.5, rather than the true value of 1/3. So, this is a biased way of computing Cohen’s <span class="math inline">\(d\)</span>.</p>
<p>The mean Cohen’s <span class="math inline">\(d\)</span> is</p>
<pre class="r"><code>mean(sim_data$cohensd)</code></pre>
<pre><code>## [1] 0.5225447</code></pre>
<p>Now, let’s compare that to the case that the test has power of 0.8.</p>
<pre class="r"><code>power.t.test(delta = 1, sd = 3, sig.level = .05, power = 0.8, type = &quot;one&quot;)</code></pre>
<pre><code>## 
##      One-sample t test power calculation 
## 
##               n = 72.58407
##           delta = 1
##              sd = 3
##       sig.level = 0.05
##           power = 0.8
##     alternative = two.sided</code></pre>
<pre class="r"><code>sim_data &lt;- do.call(rbind.data.frame, lapply(1:10000, function(x) {
  dat &lt;- rnorm(73, 1, 3)
  if(t.test(dat)$p.value &lt; .05) { 
       data.frame(mean = mean(dat), sd = sd(dat))
  } else {
       data.frame(mean = NA, sd = NA)
  }
})) %&gt;% tidyr::drop_na()
sim_data &lt;- mutate(sim_data, cohensd = mean/sd)
hist(sim_data$cohensd)</code></pre>
<p><img src="/post/2018-11-09-on-power-and-effect-size_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<pre class="r"><code>mean(sim_data$cohensd)</code></pre>
<pre><code>## [1] 0.3783282</code></pre>
<p>Still biased, but now the most likely outcome is a value that is approximately correct.</p>
<p>Finally, we show an example where the test is really underpowered; that is, <span class="math inline">\(H_0\)</span> is actually true.</p>
<pre class="r"><code>sim_data &lt;- do.call(rbind.data.frame, lapply(1:10000, function(x) {
  dat &lt;- rnorm(100, 0, 3)
  if(t.test(dat)$p.value &lt; .05) { 
       data.frame(mean = mean(dat), sd = sd(dat))
  } else {
       data.frame(mean = NA, sd = NA)
  }
})) %&gt;% tidyr::drop_na()
sim_data &lt;- mutate(sim_data, cohensd = mean/sd)
hist(sim_data$cohensd)</code></pre>
<p><img src="/post/2018-11-09-on-power-and-effect-size_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<pre class="r"><code>mean(abs(sim_data$cohensd))</code></pre>
<pre><code>## [1] 0.2380721</code></pre>
</div>
<div id="sample-size" class="section level3">
<h3>Sample size</h3>
<p>I haven’t done the math on how power and bias in estimattion effect size are related. I tought, though, that the sample size might also play a roll. So, I ran an analysis of <em>relative</em> error in effect size estimate for different sample sizes and different powers. Then, I plotted them below. You can see the (ugly) code I used to do this on my <a href="https://github.com/speegled/blog">github account</a> if you are so inclined. Bottom line, turns out I was wrong. The bias in the estimation of effect size seems to only depend on the power of the test. The standard error in the estimation of the effect size will depend on the sample size, though, so there is still a benefit to having a larger sample size even with the same power.</p>
<p><img src="/post/2018-11-09-on-power-and-effect-size_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
</div>
