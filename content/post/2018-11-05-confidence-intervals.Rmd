---
title: "CI for mean assumptions"
author: "Darrin Speegle"
date: '2018-11-05'
slug: ci-for-mean
tags:
- confidence interval
- simulation
- assumptions
- normality
categories: []
---

### Introduction

In this post, we examine the assumption of confidence intervals for the mean. Recall that the assumptions needed for `t.test` to give accurate confidence intervals is that $X_1,\ldots,X_n$ are iid normal random variables. That is, we assume that our data is a random sample from a normal population with (presumably) unknown mean and variance.

We are going to use simulation to examine this. Let's start by simulating a random sample of size 20 from a normal population with mean 1 and standard deviation 2. We'll compute a 95% confidence interval, and see whether the true mean of 2 is in it.

```{r}
set.seed(1162018)
dat <- rnorm(20, 1, 2)
a <- t.test(dat)
a
```
Yes, it is. Just barely.

Now, we will want to repeat this many times. In order to do this, we'll need to pull out the lower and upper confidence bounds. I'll use the `tidy` function from `broom` (I'll also show you a way that works faster).

```{r, message = FALSE, warning = FALSE}
library(dplyr)
a <- a %>% broom::tidy()
str(a)
```

Now, we see that `a$conf.low` is the lower confidence bound, and `a$conf.high` is the upper confidence bound. We can test whether 1 is between those, and replicate.

```{r, cache = TRUE}
mean(replicate(10000, {
  dat <- rnorm(20, 1, 2)
  a <- t.test(dat) %>% broom::tidy()
  a$conf.low < 1 && a$conf.high > 1
}))
```
We get that 95.1% of the 10000 confidence intervals contain the true mean of 1. So, the confidence intervals are performing as deisgned. 

As a reminder, this doesn't work as well for skew distributions such as exponential. (I'm switching over to the faster way now.)

```{r}
bb <- mean(replicate(10000, {
  dat <- rexp(20)
  a <- t.test(dat) 
  a$conf.int[1] < 1 && a$conf.int[2] > 1
}))
```

Here is the slower way; I didn't run it.
```{r, eval = FALSE}
mean(replicate(10000, {
  dat <- rexp(1)
  a <- t.test(dat) %>% broom::tidy()
  a$conf.low < 1 && a$conf.high > 1
}))
```

Here, we get that `r round(100 * bb, 1)`% of the 10000 confidence intervals contain the true mean of 1. This means that the test is **not** working as deisgned. 

As a reminder, when the distribution is skew, *smaller* confidence intervals work worse. Here is an example when we are trying to create 99.5% confidence intervals.


```{r}
bb <- mean(replicate(10000, {
  dat <- rexp(20)
  a <- t.test(dat, conf.level = 0.995) 
  a$conf.int[1] < 1 && a$conf.int[2] > 1
}))
```

Slower way:
```{r, eval = FALSE}
mean(replicate(10000, {
  dat <- rexp(1)
  a <- t.test(dat, conf.level = 0.995) %>% broom::tidy()
  a$conf.low < 1 && a$conf.high > 1
}))
```

Now, `r round(100 * bb, 1)` percent of the 10000 confidence intervals contain the true mean. So, instead of a 0.5% error rate, we have a `r round(100 - 100*bb, 1)` percent error rate.

### Testing for normality

I teach my students to plot their data before using `t.test`. I think they need to look at it and see whether the data seemingly comes from a normal population. However, I don't think this is a statistically valid approach. Let's think through what we are doing.

Our full workflow is that we test for normality, then do a t test if the data is normal. If the data is not normal, then we do some other method of creating confidence intervals. The question is: when I do this entire procedure, what percentage of times does the confidence interval contain the true mean? For the sake of argument, let's say that we are doing a 95% confidence interval.

```{r}
dat <- rexp(20)
if(shapiro.test(dat)$p.value > .05) {
  a <- t.test(dat, mu = 1)$p.value
} else {
  a <- NA
}
a
```

```{r, cache = TRUE}
sim_data <- replicate(10000, {
  dat <- rexp(20)
  if(shapiro.test(dat)$p.value > .05) {
    a <- t.test(dat, mu = 1)$p.value
  } else {
    a <- NA
  }
  a
})
bb <- mean(sim_data < .05, na.rm = TRUE)
```
Now, note that the 95% confidence interval contains the true mean only `r 100 - round(100*bb, 1)`% of the time! So, testing the data for normality made the situation *worse*. Without testing for normality, we got that a 95% confidence interval contained the true mean about 91.7% of the time!

Of course, it could be that we aren't really doing many tests. Let's see the percentage of times that we performed `t.test`.
```{r}
mean(is.na(sim_data))
```
Indeed, only about 1/6 of the time does our data even pass the normality test. Now it becomes really important to imagine what we are going to do if the normality test fails.

So, let's imagine that our full workflow is to 

  1. Test for normality
  2. If normality test passes, then do `t.test`
  3. If normality test fails, then do a bootstrap confidence interval.

For our purposes, this is the bootstrap confidence interval that we'll be doing. Perhaps there is a better general procedure? I am assuming that we don't know what kind of process generates the data. Let's see how well the bootstrap confidence interval I picked does on its own.

```{r, cache = TRUE, warning = FALSE, message = FALSE}
library(boot)
m <- function(x, y) mean(x[y])
mean(replicate(1000, {
  dat <- rexp(20)
  b <- boot(data = dat, R = 500, statistic = m, sim = "ordinary")
  b <- boot.ci(b, type = "bca")
  a <- ((b$bca[1,4] < 1) && (b$bca[1,5] > 1))
  a
}))
```

Not too bad. Better than just doing `t.test` on the data, so I think we can say that it is a reasonable approach.

Now, let's do our whole, combined algorithm:

```{r, message = FALSE, warning = FALSE, cache = TRUE}
sim_data <- replicate(1000, {
  dat <- rexp(15)
  if(shapiro.test(dat)$p.value > .2) {
    a <- t.test(dat, mu = 1)$p.value > .05
  } else {
    b <- boot(data = dat, R = 500, statistic = m, sim = "ordinary")
    b <- boot.ci(b, type = "bca")
    a <- ((b$bca[1,4] < 1) && (b$bca[1,5] > 1))
  }
  a
})
mean(sim_data)
```

Oh, dear. This turns out to be a *terrible* idea. Our 95% confidence interval would have been an effective 92% confidence interval by just doing the bootstrap confidence interval. It would have been an effective 92% confidence interval had we simply applied `t.test` without checking for normality. However, after checking for normality and applying `t.test` to the data that appears normal and `boot.ci` to the data that doesn't, we end up with an effective 80% confidence interval.

### What in the name of all things poopy is going on?

Let's look more carefully at the data that passes `shapiro.test` and the data that doesn't. I'll simulate two sampling distributions of the mean --- one for each type.

```{r}
plot_data <- data.frame(mean_dat = numeric(0),
                        sd_dat = numeric(0), 
                        pass_shaprio = logical(0))

for(i in 1:10000) {
  dat <- rexp(20)
  if(shapiro.test(dat)$p.value > .05) {
    plot_data <- rbind(plot_data, list(mean_dat = mean(dat), 
                                       sd_dat = sd(dat),
                                       pass_shapiro = TRUE))
  } else {
    plot_data <- rbind(plot_data, list(mean_dat = mean(dat), 
                                       sd_dat = sd(dat),
                                       pass_shapiro = FALSE))
  }
}
```

```{r, warning = FALSE, message = FALSE}
library(ggplot2)
ggplot(plot_data, aes(x = sd_dat)) +
  geom_histogram() +
  facet_wrap(~pass_shapiro, scales = "free_y")
plot_data %>% 
  group_by(pass_shapiro) %>% 
  summarize(m = mean(mean_dat), s = mean(sd_dat))
```


### Summary

1. When estimating the effective type I error rate, one should take into account the entire workflow of the test. 
2. It is dangerous to do *ad hoc* combinations of tests; even when they may seem like good ideas.